<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="https://jc5201.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://jc5201.github.io/" rel="alternate" type="text/html" /><updated>2022-01-25T13:12:01+09:00</updated><id>https://jc5201.github.io/feed.xml</id><title type="html">NGD’s blog</title><subtitle></subtitle><author><name>Jaechang Kim</name><email>jaechang@postech.ac.kr</email></author><entry><title type="html">Music Source Separation in the waveform domain</title><link href="https://jc5201.github.io/paper%20-%20audio/paper-demucs/" rel="alternate" type="text/html" title="Music Source Separation in the waveform domain" /><published>2021-02-05T00:00:00+09:00</published><updated>2021-02-05T00:00:00+09:00</updated><id>https://jc5201.github.io/paper%20-%20audio/paper-demucs</id><content type="html" xml:base="https://jc5201.github.io/paper%20-%20audio/paper-demucs/">&lt;h3 id=&quot;intro&quot;&gt;intro&lt;/h3&gt;

&lt;p&gt;facebook ai 쪽에 있는 defossez라는 아저씨 논문이다.&lt;/p&gt;

&lt;p&gt;Demucs 라고 다른 논문들에서 baseline으로 주로 제시되는 모델 중 하나인데 publish된 적은 없는 것 같다.&lt;/p&gt;

&lt;p&gt;데모부터 들어보면 상당히 괜찮은 결과를 보여준다. https://ai.honu.io/papers/demucs/index.html&lt;/p&gt;

&lt;p&gt;작년에 wave u net 가지고 씨름하지 말고 이 모델을 바로 시도했으면 어땠을까 싶다.&lt;/p&gt;

&lt;h3 id=&quot;content&quot;&gt;content&lt;/h3&gt;

&lt;p&gt;푸는 문제는 전형적인 music source separation 문제다. mixture signal을 (1)drums, (2)bass, (3)other, (4)vocals로 구분한다.&lt;/p&gt;

&lt;p&gt;waveform domain에서 문제를 풀기 위해서 크게 두 가지 방법을 시도한다. 기존에 존재하는 Conv-Tasnet이라는 speech source separation 모델을 음악에 적용시키는 것과 이 연구에서 제시된 demucs라는 모델이다.&lt;/p&gt;

&lt;h4 id=&quot;adapting-conv-tasnet-for-music-source-separation&quot;&gt;Adapting Conv-Tasnet for music source separation&lt;/h4&gt;

&lt;p&gt;Conv-Tasnet을 적용하면서 생긴 변경점을 살펴보면 아래와 같다.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SI-SDR loss -&amp;gt; 이 연구에서는 L1 loss&lt;/li&gt;
  &lt;li&gt;single channel (monophonic speech separation) -&amp;gt; stereo (stereophonic music source separation)&lt;/li&gt;
  &lt;li&gt;receptive field 길이&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;L1 loss를 사용하는 이유에 대해서는 뒤에서 설명한다.&lt;/p&gt;

&lt;p&gt;이렇게 변형한 모델의 결과를 보면 여러 단점들이 있다고 한다.(a constant broadband noise, hollow instruments attacks or even missing parts.)&lt;/p&gt;

&lt;p&gt;저자가 생각하기에 이렇게 된 이유는 Conv-Tasnet이 기본적으로 masking 방식인데 악기(stem)들이 겹치면서 비가역적인 정보 손실이 일어나서 masking으로 그 손실을 복구할 수 없는 것이라고 분석했다.&lt;/p&gt;

&lt;h4 id=&quot;demucs&quot;&gt;Demucs&lt;/h4&gt;

&lt;p&gt;이제 demucs 모델 구조를 설명한다. 기본적으로 demucs는 wave-u-net 구조에서 motivated 되었다는 것을 알 수 있다. wave-u-net 구조를 기준으로 설명하자면  encoder(downsampling block) 에서 conv 뿐만 아니라 GLU(gated linear unit)을 사용하고, decoder(Upsampling block) 에서도 interpolation 대신 transposed convolution을 사용한다. bottleneck 에서는 bidirectional LSTM을 사용한다. wave-u-net 보다 convolution에서 output channel을 훨씬 깊게 만들었다. 그 이외에 skip connection 이나 전체 구조는 wave-u-net 또는 u-net과 비슷하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210205132204708.png&quot; alt=&quot;image-20210205132204708&quot; /&gt;&lt;/p&gt;

&lt;p&gt;loss function은 마찬가지로 L1 reconstruction loss를 사용한다. L1 loss function을 쓰면 phase가 다른 소리에 대해 큰 loss가 나오기 때문에 문제가 될 수 있어서 sound generation task에서 spectrogram loss를 사용하는 경우가 있다. 하지만 demucs의 경우는 모델에서 원본 소리의 phase를 알 수 있고, skip connection을 통해 쉽게 phase를 복구할 수 있기 때문에 L1/L2 loss를 사용하는 데 문제가 없다고 한다.&lt;/p&gt;

&lt;p&gt;defossez 아저씨의 이전 연구 (SING) 에서 여러 loss를 비교한 실험이 있어서 L1/l2 이외의 loss는 테스트 하지 않았다고 한다. 이전 연구에서 테스트한 loss는 L2, spectral loss 인데 multi-scale spectral loss나 SI-SDR 같은 것들에 대해서는 확인되지 않았다.&lt;/p&gt;

&lt;p&gt;그 이외에 fixup 처럼 initialization 단계에서 rescaling을 하거나 하는 단계가 있는데 구현 세부사항인 것 같아서 생략한다.&lt;/p&gt;

&lt;h4 id=&quot;result&quot;&gt;result&lt;/h4&gt;

&lt;p&gt;결과를 대략 보여주는 표가 있다. IRM oracle은 masking 방식의 upper bound로 생각하면 된다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210205133936591.png&quot; alt=&quot;image-20210205133936591&quot; /&gt;&lt;/p&gt;

&lt;p&gt;사용한 augmentation&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;다른 음악의 소스를 가져와서 새로운 mixture 생성&lt;/li&gt;
  &lt;li&gt;channel swap&lt;/li&gt;
  &lt;li&gt;phase inversion&lt;/li&gt;
&lt;/ul&gt;</content><author><name>Jaechang Kim</name><email>jaechang@postech.ac.kr</email></author><category term="paper - audio" /><summary type="html">intro</summary></entry><entry><title type="html">Unsupervised Cross-Domain Singing Voice Conversion</title><link href="https://jc5201.github.io/paper%20-%20audio/paper-Unsupervised-Cross-Domain-Singing-Voice-Conversion/" rel="alternate" type="text/html" title="Unsupervised Cross-Domain Singing Voice Conversion" /><published>2021-01-20T00:00:00+09:00</published><updated>2021-01-20T00:00:00+09:00</updated><id>https://jc5201.github.io/paper%20-%20audio/paper-Unsupervised-Cross-Domain-Singing-Voice-Conversion</id><content type="html" xml:base="https://jc5201.github.io/paper%20-%20audio/paper-Unsupervised-Cross-Domain-Singing-Voice-Conversion/">&lt;h3 id=&quot;intro&quot;&gt;intro&lt;/h3&gt;

&lt;p&gt;facebook ai 쪽 논문이고, timbre transfer 관련이라고 보면 될 것 같다.&lt;/p&gt;

&lt;p&gt;처음에 학습 시킬 때 speaker identity에 따라 다른 timbre를 표현하게 학습시키고, 이후에 다른 목소리를 표현하고 싶으면 그 목소리와 비슷한 speaker identity를 찾는 모델이다.&lt;/p&gt;

&lt;p&gt;speaker identity를 찾는 과정은 speech 데이터에서도 찾을 수 있고, 상대적으로 적은 데이터로 할 수 있다.&lt;/p&gt;

&lt;p&gt;GAN 에서 G에 condition을 다양하게 줘서 노래에 대한 정보나 speaker에 대한 정보를 넘겨주고, 소리를 생성하게 하는 모델을 사용한다.&lt;/p&gt;

&lt;h3 id=&quot;content&quot;&gt;content&lt;/h3&gt;

&lt;p&gt;singing voice conversion 은 input으로 들어온 노래를 다른 사람의 목소리로 reproduce 하는 문제다. 이 논문의 결과는 https://singing-conversion.github.io/ 에서 들어볼 수 있다. 각 데이터에는 사람 1명의 목소리만 있다.&lt;/p&gt;

&lt;p&gt;GAN을 사용하면서 G 에 parameter로 여러 가지를 넘겨주는데, speaker-invariant feature들과 speaker identity다.&lt;/p&gt;

&lt;p&gt;speaker-invariant feature 로는 loudness, f0를 사용한다. loudness는 log-scaled power spectrum을 사용하고, f0는 CREPE를 사용해서 추출한다. f0를 그대로 사용하면 생성된 소리의 pitch가 불안정해서 single sinusoid sine-exitation $\Gamma (f_{crepe}(x))$ 를 사용한다. Speech의 경우 Wav2Letter라는 음성인식 쪽 모델을 가져와서 추가로 사용한다.&lt;/p&gt;

&lt;p&gt;Speaker identity에 관해서는 논문에 자세히 나와있지는 않은데 &lt;a href=&quot;https://arxiv.org/abs/1904.06590&quot;&gt;이전 논문&lt;/a&gt; 을 참고했을 때 embedding 으로 이해하면 될 것 같다.&lt;/p&gt;

&lt;p&gt;모델은 기본적으로 Least-squares GAN 을 따르고 있다. 학습 목표는 크게 두 가지로 나누는데, single singer objective function 세팅과 multi-singer 세팅이다.&lt;/p&gt;

&lt;p&gt;Single singer objective function 의 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120170012346.png&quot; alt=&quot;image-20210120170012346&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120170027904.png&quot; alt=&quot;image-20210120170027904&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;G의 objective는 여러 loss의 합으로 표현되어 있는데, adv loss 는 GAN에서 일반적으로 사용하는 objective로 볼 수 있다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120170501191.png&quot; alt=&quot;image-20210120170501191&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;recon loss는 $\hat{x}$ 이 x를 얼마나 잘 reconstruciton 했는지 비교하는 것으로, multi-scale spectral loss를 사용한다. 아래 식에서 F 는 Frobenius norm인데, l2 norm이다. 첫 번째 term이 spectral peak 를 표현하고, 두 번째 term이 소리의 silent section에 대한 penalty라고 한다. 두 번째 term이 어떻게 그런 의미를 갖는지는 잘 모르겠다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120170921416.png&quot; alt=&quot;image-20210120170921416&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120171248978.png&quot; alt=&quot;image-20210120171248978&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;나머지 두 loss는 생성된 소리의 quality를 위한 부분이고, feature extractor로 사용한 모델들의 중간 layer의 activation을 사용한다. perceptual loss의 아이디어를 가져온 것 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120171455546.png&quot; alt=&quot;image-20210120171455546&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Multi-singer 세팅에서는 G에 feature 외에 speaker identity를 추가한다. 예를 들어, speaker i의 샘플을 speaker i의 소리로 recon 했을 때와 speaker j의 소리로 recon했을 때 아래와 같이 표기한다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120171837720.png&quot; alt=&quot;image-20210120171837720&quot; style=&quot;zoom: 50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120171849436.png&quot; alt=&quot;image-20210120171849436&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;speaker 를 바꿨을 때는 recon loss를 포함시키지 않는 것을 제외하면 objective는 동일하다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120172450192.png&quot; alt=&quot;image-20210120172450192&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;speaker identity의 convex combination을 통해 virtual speaker를 생성했다. 이 세팅을 mixup 세팅이라고 부르는데 특별한 의미가 있는지 잘 모르겠다. 다양한 speaker에 대해 결과가 잘 나오게 하는데 도움이 될 것 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120172658222.png&quot; alt=&quot;image-20210120172658222&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120172706001.png&quot; alt=&quot;image-20210120172706001&quot; style=&quot;zoom:50%;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;모델 구조는 non-causal WaveNet 기반이고, auto-regressive하지 않기 때문에 빠르다고 한다. 자세한 모델 구조에 대해서는 적지 않겠다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120172903740.png&quot; alt=&quot;image-20210120172903740&quot; /&gt;&lt;/p&gt;

&lt;p&gt;결과는  https://singing-conversion.github.io/  에 여러 샘플들이 있으니 바로 들어보는게 빠른 것 같다. Speech에서 학습한 speaker identity로 conversion을 하는 내용들도 포함되어 있다.&lt;/p&gt;

&lt;p&gt;evaluation 결과는 아래와 같다.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/img/image-20210120173951649.png&quot; alt=&quot;image-20210120173951649&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;conclusion&quot;&gt;conclusion&lt;/h3&gt;

&lt;p&gt;f&lt;/p&gt;</content><author><name>Jaechang Kim</name><email>jaechang@postech.ac.kr</email></author><category term="paper - audio" /><summary type="html">intro</summary></entry><entry><title type="html">ubuntu GUI setting</title><link href="https://jc5201.github.io/setting/i3-setting/" rel="alternate" type="text/html" title="ubuntu GUI setting" /><published>2020-11-14T00:00:00+09:00</published><updated>2020-11-14T00:00:00+09:00</updated><id>https://jc5201.github.io/setting/i3-setting</id><content type="html" xml:base="https://jc5201.github.io/setting/i3-setting/">&lt;p&gt;사용 환경 : virtualbox + ubuntu server 20.04 + i3 + st-luke&lt;/p&gt;

&lt;h3 id=&quot;i3&quot;&gt;i3&lt;/h3&gt;

&lt;p&gt;순정 i3를 쓸 거면 apt 사용해서 그냥 설치하면 된다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo apt install i3
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;난 i3-gaps를 쓸 것이기 때문에 i3-gaps를 빌드해서 설치했고, 다음 페이지를 참고했다.&lt;/p&gt;

&lt;p&gt;https://dymaxionkim.github.io/beautiful-jekyll/2019-06-20-rounded-i3-gaps-on-ubuntu/&lt;/p&gt;

&lt;p&gt;https://gist.github.com/boreycutts/6417980039760d9d9dac0dd2148d4783&lt;/p&gt;

&lt;p&gt;i3 설정은 ~/.config/i3/config 에서 하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;st-luke&quot;&gt;st-luke&lt;/h3&gt;

&lt;p&gt;디폴트로 깔려있는 터미널 쓰기 싫어서 st 를 깔았는데 scrollback이나 기본적인 세팅이 되어있는 버전이 있어서 그걸 사용했다.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git clone https://github.com/LukeSmithxyz/st.git
cd st
apt install libx11-dev fontconfig libxft-dev libharfbuzz-dev
sudo make install
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;설정은 config.h를 수정하고 새로 빌드하면 된다.&lt;/p&gt;

&lt;h3 id=&quot;vm&quot;&gt;vm&lt;/h3&gt;

&lt;p&gt;virtualbox에서 clipboard 공유 기능이 작동하지 않을 것인데 아래 방법으로 설치하고 재부팅하면 VBoxClient가 설치되어 있을 것이고 VBoxClient –clipboard를 하면 클립보드가 작동할 것이다. /dev/cdrom은 버추얼박스에서 게스트 확장 설치를 누르면 생긴다.&lt;/p&gt;

&lt;p&gt;작동이 안된다면 st 설정에서 ctrl+insert 로 복사하는 키 설정을 했는지 확인해보자.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sudo mkdir /mnt/cdrom
sudo mount /dev/cdrom /mnt/cdrom 
sudo sh ./VBoxLinuxAdditions.run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;이후-설정&quot;&gt;이후 설정&lt;/h3&gt;

&lt;p&gt;polybar 같은 건 필요한 경우 찾아서 설치하면 된다.&lt;/p&gt;

&lt;p&gt;startx 로 i3 실행할 때 VBoxClient-all 실행하고 단축키 설정하고 그런 건 적당히 찾아가면서 하면 된다.&lt;/p&gt;

&lt;p&gt;내 설정 파일은 https://github.com/jc5201/configs 에 있다.&lt;/p&gt;</content><author><name>Jaechang Kim</name><email>jaechang@postech.ac.kr</email></author><category term="setting" /><summary type="html">사용 환경 : virtualbox + ubuntu server 20.04 + i3 + st-luke</summary></entry><entry><title type="html">Jekyll blog test</title><link href="https://jc5201.github.io/random/test/" rel="alternate" type="text/html" title="Jekyll blog test" /><published>2019-10-31T00:00:00+09:00</published><updated>2019-10-31T00:00:00+09:00</updated><id>https://jc5201.github.io/random/test</id><content type="html" xml:base="https://jc5201.github.io/random/test/">&lt;h5 id=&quot;title&quot;&gt;title&lt;/h5&gt;
&lt;p&gt;content
—–
‘’’
code
‘’’&lt;/p&gt;</content><author><name>Jaechang Kim</name><email>jaechang@postech.ac.kr</email></author><category term="random" /><summary type="html">title content —– ‘’’ code ‘’’</summary></entry></feed>